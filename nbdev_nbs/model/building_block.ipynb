{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model.building_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Building Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "The building block module specifies the architectures of the core neural networks used in PeptDeep.\n",
    "\n",
    "All networks are based on the [PyTorch]('https://pytorch.org/') package by subclassing `torch.nn.Module`, which is the base class for all neural networks. To implement the Transformer-network, the HuggingFace [transformers]('https://huggingface.co/docs/transformers/') package is used, which allows to specify transformer networks in Pytorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import numpy as np\n",
    "#BERT from huggingface\n",
    "from transformers.models.bert.modeling_bert import BertEncoder\n",
    "\n",
    "from peptdeep.model.featurize import (\n",
    "    mod_feature_size\n",
    ")\n",
    "from peptdeep.settings import model_const\n",
    "from peptdeep.settings import global_settings as settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "torch.set_num_threads(2)\n",
    "\n",
    "max_instrument_num = model_const['max_instrument_num']\n",
    "frag_types = settings['model']['frag_types']\n",
    "max_frag_charge = settings['model']['max_frag_charge']\n",
    "num_ion_types = len(frag_types)*max_frag_charge\n",
    "aa_embedding_size = model_const['aa_embedding_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "Basic embeddings or encodings of inputs such as AA sequence or modification state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def aa_embedding(hidden_size, aa_embedding_size=aa_embedding_size):\n",
    "    return torch.nn.Embedding(aa_embedding_size, hidden_size, padding_idx=0)\n",
    "\n",
    "def ascii_embedding(hidden_size):\n",
    "    return torch.nn.Embedding(128, hidden_size, padding_idx=0)\n",
    "\n",
    "def aa_one_hot(aa_indices, *cat_others, aa_embedding_size=aa_embedding_size):\n",
    "    aa_x = torch.nn.functional.one_hot(\n",
    "        aa_indices, aa_embedding_size\n",
    "    )\n",
    "    return torch.cat((aa_x, *cat_others), 2)\n",
    "\n",
    "def instrument_embedding(hidden_size):\n",
    "    return torch.nn.Embedding(max_instrument_num, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial states\n",
    "Generates tensors defining the initial (hidden) states of the elements in the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def zero_param(*shape):\n",
    "    return torch.nn.Parameter(torch.zeros(shape), requires_grad=False)\n",
    "\n",
    "def xavier_param(*shape):\n",
    "    x = torch.nn.Parameter(torch.empty(shape), requires_grad=False)\n",
    "    torch.nn.init.xavier_uniform_(x)\n",
    "    return x\n",
    "\n",
    "init_state = xavier_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks for sequences\n",
    "The seq networks take the sequence and possible accompanying information such as charge state or modification and apply neural network transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SeqCNN_MultiKernel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Extract sequence features using `torch.nn.Conv1D` with \n",
    "    different kernel sizes (1(residue connection),3,5,7), \n",
    "    and then concatenate the outputs of these Conv1Ds.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_features:int):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        out_features : int\n",
    "            Must be divided by 4.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            \"out_features must be divided by 4\"\n",
    "            \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        hidden = out_features//4\n",
    "        if hidden*4 != out_features:\n",
    "            raise ValueError('out_features must be divided by 4')\n",
    "\n",
    "        self.cnn_short = torch.nn.Conv1d(\n",
    "            hidden, hidden,\n",
    "            kernel_size=3, padding=1\n",
    "        )\n",
    "        self.cnn_medium = torch.nn.Conv1d(\n",
    "            hidden, hidden,\n",
    "            kernel_size=5, padding=2\n",
    "        )\n",
    "        self.cnn_long = torch.nn.Conv1d(\n",
    "            hidden, hidden,\n",
    "            kernel_size=7, padding=3\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x1 = self.cnn_short(x)\n",
    "        x2 = self.cnn_medium(x)\n",
    "        x3 = self.cnn_long(x)\n",
    "        return torch.cat((x, x1, x2, x3), dim=1).transpose(1,2)\n",
    "\n",
    "#legacy\n",
    "class SeqCNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Extract sequence features using `torch.nn.Conv1D` with \n",
    "    different kernel sizes (1(residue connection),3,5,7), and then concatenate \n",
    "    the outputs of these Conv1Ds. The Output dim is 4*embedding_hidden.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn_short = torch.nn.Conv1d(\n",
    "            embedding_hidden, embedding_hidden,\n",
    "            kernel_size=3, padding=1\n",
    "        )\n",
    "        self.cnn_medium = torch.nn.Conv1d(\n",
    "            embedding_hidden, embedding_hidden,\n",
    "            kernel_size=5, padding=2\n",
    "        )\n",
    "        self.cnn_long = torch.nn.Conv1d(\n",
    "            embedding_hidden, embedding_hidden,\n",
    "            kernel_size=7, padding=3\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x1 = self.cnn_short(x)\n",
    "        x2 = self.cnn_medium(x)\n",
    "        x3 = self.cnn_long(x)\n",
    "        return torch.cat((x, x1, x2, x3), dim=1).transpose(1,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Built-in Transformers (for test only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Seq_Transformer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Using PyTorch built-in Transformer layers\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        in_features,\n",
    "        hidden_features,\n",
    "        nheads=8,\n",
    "        nlayers=2,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(\n",
    "            in_features, nheads, hidden_features, dropout\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
    "            encoder_layers, nlayers\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.transformer_encoder(x.permute(1,0,2)).permute(1,0,2)\n",
    "\n",
    "\n",
    "class Hidden_Transformer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer NN based on pytorch's built-in TransformerLayer class\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        hidden, hidden_expand=4,\n",
    "        nheads=8, nlayers=4, dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.transormer = Seq_Transformer(\n",
    "            hidden, hidden*hidden_expand, nheads=nheads, \n",
    "            nlayers=nlayers, dropout=dropout\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.transormer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class _Pseudo_Bert_Config:\n",
    "    def __init__(self, \n",
    "        hidden_dim=256, \n",
    "        intermediate_size=1024,\n",
    "        num_attention_heads=8,\n",
    "        num_bert_layers=4,\n",
    "        dropout=0.1,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self.add_cross_attention = False\n",
    "        self.chunk_size_feed_forward = 0\n",
    "        self.is_decoder = False\n",
    "        self.seq_len_dim = 1\n",
    "        self.training = False\n",
    "        self.hidden_act = \"gelu\"\n",
    "        self.hidden_dropout_prob = dropout\n",
    "        self.attention_probs_dropout_prob = dropout\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.initializer_range = 0.02\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.layer_norm_eps = 1e-8\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_hidden_layers = num_bert_layers\n",
    "        self.output_attentions = output_attentions\n",
    "\n",
    "class Hidden_HFace_Transformer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer NN based on HuggingFace's BertEncoder class\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        hidden_dim, hidden_expand=4,\n",
    "        nheads=8, nlayers=4, dropout=0.1,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = _Pseudo_Bert_Config(\n",
    "            hidden_dim=hidden_dim,\n",
    "            intermediate_size=hidden_dim*hidden_expand,\n",
    "            num_attention_heads=nheads,\n",
    "            num_bert_layers=nlayers,\n",
    "            dropout=dropout,\n",
    "            output_attentions=False\n",
    "        )\n",
    "        self.output_attentions = output_attentions\n",
    "        self.bert = BertEncoder(self.config)\n",
    "    def forward(self, x:torch.Tensor)->tuple:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            (Tensor, [Tensor]): out[0] is the hidden layer output, \n",
    "              and out[1] is the output attention \n",
    "              if self.output_attentions==True\n",
    "        \"\"\"\n",
    "        return self.bert(\n",
    "            x,\n",
    "            output_attentions=self.output_attentions,\n",
    "            return_dict=False\n",
    "        )\n",
    "#legacy\n",
    "HiddenBert = Hidden_HFace_Transformer\n",
    "\n",
    "class HFace_Transformer_with_PositionalEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    HuggingFace transformer with a positional encoder in front.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_dim : int\n",
    "        Input and output feature dimension.\n",
    "\n",
    "    hidden_expand : int, optional\n",
    "        FFN hidden size = hidden*hidden_expand. Defaults to 4.\n",
    "\n",
    "    nhead : int, optional\n",
    "        Multi-head attention number. Defaults to 8.\n",
    "\n",
    "    nlayers : int, optional\n",
    "        Number of transformer layers. Defaults to 4.\n",
    "\n",
    "    dropout : float, optional\n",
    "        Dropout rate. Defaults to 0.1.\n",
    "\n",
    "    output_attentions : bool, optional\n",
    "        If output attention values. Defaults to False.\n",
    "\n",
    "    max_len : int, optional\n",
    "        Max input sequence length. Defaults to 200.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        hidden_dim:int, hidden_expand=4,\n",
    "        nheads=8, nlayers=4, dropout=0.1,\n",
    "        output_attentions=False,\n",
    "        max_len=200,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim, max_len=max_len)\n",
    "        self.bert = Hidden_HFace_Transformer(\n",
    "            hidden_dim=hidden_dim, hidden_expand=hidden_expand,\n",
    "            nheads=nheads, nlayers=nlayers, dropout=dropout,\n",
    "            output_attentions=output_attentions\n",
    "        )\n",
    "    def forward(self, x:torch.Tensor)->tuple:\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            Tensor: Output tensor.\n",
    "            [Tensor]: Attention tensor, returned only if output_attentions is True.\n",
    "        \"\"\"\n",
    "        x = self.pos_encoder(x)\n",
    "        return self.bert(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SeqLSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    returns LSTM applied on sequence input\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 rnn_layer=2, bidirectional=True\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        if bidirectional:\n",
    "            if out_features%2 != 0:\n",
    "                raise ValueError(\"'out_features' must be able to be divided by 2\")\n",
    "            hidden = out_features//2\n",
    "        else:\n",
    "            hidden = out_features\n",
    "\n",
    "        self.rnn_h0 = init_state(\n",
    "            rnn_layer+rnn_layer*bidirectional,\n",
    "            1, hidden\n",
    "        )\n",
    "        self.rnn_c0 = init_state(\n",
    "            rnn_layer+rnn_layer*bidirectional,\n",
    "            1, hidden\n",
    "        ) \n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size = in_features,\n",
    "            hidden_size = hidden,\n",
    "            num_layers = rnn_layer,\n",
    "            batch_first = True,\n",
    "            bidirectional = bidirectional,\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        h0 = self.rnn_h0.repeat(1, x.size(0), 1)\n",
    "        c0 = self.rnn_c0.repeat(1, x.size(0), 1)\n",
    "        x, _ = self.rnn(x, (h0,c0))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SeqGRU(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    returns GRU applied on sequence input\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 rnn_layer=2, bidirectional=True\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        if bidirectional:\n",
    "            if out_features%2 != 0:\n",
    "                raise ValueError(\"'out_features' must be able to be divided by 2\")\n",
    "            # to make sure that output dim is out_features\n",
    "            # as `bidirectional` will cat forward and reverse RNNs\n",
    "            hidden = out_features//2\n",
    "        else:\n",
    "            hidden = out_features\n",
    "        \n",
    "        self.rnn_h0 = init_state(\n",
    "            rnn_layer+rnn_layer*bidirectional, \n",
    "            1, hidden\n",
    "        )\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size = in_features,\n",
    "            hidden_size = hidden,\n",
    "            num_layers = rnn_layer,\n",
    "            batch_first = True,\n",
    "            bidirectional = bidirectional,\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        h0 = self.rnn_h0.repeat(1, x.size(0), 1)\n",
    "        x, _ = self.rnn(x, h0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Seq Transformations\n",
    "\n",
    "Takes in a sequence and applies a linear transformation on it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SeqAttentionSum(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    apply linear transformation and tensor rescaling with softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.attn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features, 1, bias=False),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn = self.attn(x)\n",
    "        return torch.sum(torch.mul(x, attn), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Encoding and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    transform sequence input into a positional representation\n",
    "    \"\"\"\n",
    "    def __init__(self, out_features=128, max_len = 200):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(\n",
    "                0, out_features, 2\n",
    "            ) * (-np.log(max_len) / out_features)\n",
    "        )\n",
    "        pe = torch.zeros(1, max_len, out_features)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:,:x.size(1),:]\n",
    "\n",
    "class PositionalEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    transform sequence with the standard embedding function\n",
    "    \"\"\"\n",
    "    def __init__(self, out_features=128, max_len=200):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_emb = torch.nn.Embedding(\n",
    "            max_len, out_features\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        return x + self.pos_emb(torch.arange(\n",
    "            x.size(1), dtype=torch.long, device=x.device\n",
    "        ).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Networks\n",
    "The 'Input' classes represent the input layers of the networks, meaning they interact directly with the (formatted) features such as the peptide sequence, the charge state, the modifications or the collision energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear input transformations and embeddings.\n",
    "Performing embedding and linear operations on the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Meta_Embedding(torch.nn.Module):\n",
    "    \"\"\"Encodes Charge state, Normalized Collision Energy (NCE) and Instrument for a given spectrum \n",
    "    into a 'meta' single layer network\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        out_features,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.nn = torch.nn.Linear(\n",
    "            max_instrument_num+1, out_features-1\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "        charges, NCEs, instrument_indices,\n",
    "    ):\n",
    "        inst_x = torch.nn.functional.one_hot(\n",
    "            instrument_indices, max_instrument_num\n",
    "        )\n",
    "        meta_x = self.nn(torch.cat((inst_x, NCEs), 1))\n",
    "        meta_x = torch.cat((meta_x, charges), 1)\n",
    "        return meta_x\n",
    "#legacy\n",
    "InputMetaNet = Meta_Embedding\n",
    "\n",
    "class Mod_Embedding_FixFirstK(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes the modification vector in a single layer feed forward network, but not transforming the first k features\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    out_features: int\n",
    "        how many features to output\n",
    "    k: int\n",
    "        how many first features to keep, defaults to 6 (the core organic elements: C, H, N, O, P, S)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        out_features:int,\n",
    "        k:int = 6, *,\n",
    "        mod_features:int = mod_feature_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.nn = torch.nn.Linear(\n",
    "            mod_features-self.k, out_features-self.k,\n",
    "            bias=False\n",
    "        ) if out_features > k else torch.nn.Identity()\n",
    "\n",
    "    def forward(self,\n",
    "        mod_x,\n",
    "    ):\n",
    "        return mod_x if mod_x.shape[2] <= self.k \\\n",
    "            else torch.cat((\n",
    "                mod_x[:,:,:self.k],\n",
    "                self.nn(mod_x[:,:,self.k:])\n",
    "            ), 2)\n",
    "#legacy\n",
    "InputModNetFixFirstK = Mod_Embedding_FixFirstK\n",
    "\n",
    "class AA_Mod_Embedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Concatenates the AA (128 ASCII codes) embedding with the modifcation vector\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    out_features: int\n",
    "        modification embeding size\n",
    "    mod_features: int\n",
    "        how many first modification features to keep\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        out_features,\n",
    "        mod_features = 8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mod_embedding = Mod_Embedding_FixFirstK(\n",
    "            mod_features\n",
    "        )\n",
    "        self.aa_embedding = ascii_embedding(\n",
    "            out_features-mod_features\n",
    "        )\n",
    "    def forward(self, aa_indices, mod_x):\n",
    "        mod_x = self.mod_embedding(mod_x)\n",
    "        aa_x = self.aa_embedding(aa_indices)\n",
    "        return torch.cat((aa_x, mod_x), 2)\n",
    "#legacy\n",
    "InputAAEmbedding = AA_Mod_Embedding\n",
    "\n",
    "class Mod_Embedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes the modification vector in a single layer feed forward network\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        out_features,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.nn = torch.nn.Linear(\n",
    "            mod_feature_size, out_features,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "        mod_x,\n",
    "    ):\n",
    "        return self.nn(mod_x)\n",
    "#legacy\n",
    "InputModNet = Mod_Embedding\n",
    "\n",
    "class Input_26AA_Mod_PositionalEncoding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes AA (26 AA letters) and modification vector\n",
    "    \"\"\"\n",
    "    def __init__(self, out_features, max_len=200):\n",
    "        super().__init__()\n",
    "        mod_hidden = 8\n",
    "        self.mod_nn = Mod_Embedding_FixFirstK(mod_hidden)\n",
    "        self.aa_emb = aa_embedding(\n",
    "            out_features-mod_hidden\n",
    "        )\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            out_features, max_len\n",
    "        )\n",
    "        \n",
    "    def forward(self, \n",
    "        aa_indices, mod_x\n",
    "    ):\n",
    "        mod_x = self.mod_nn(mod_x)\n",
    "        x = self.aa_emb(aa_indices)\n",
    "        return self.pos_encoder(torch.cat((x, mod_x), 2))\n",
    "#legacy\n",
    "AATransformerEncoding = Input_26AA_Mod_PositionalEncoding\n",
    "\n",
    "class Input_AA_Mod_PositionalEncoding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes AA (ASCII codes) and modification vector\n",
    "    \"\"\"\n",
    "    def __init__(self, out_features, max_len=200):\n",
    "        super().__init__()\n",
    "        mod_hidden = 8\n",
    "        self.mod_nn = Mod_Embedding_FixFirstK(mod_hidden)\n",
    "        self.aa_emb = ascii_embedding(\n",
    "            out_features-mod_hidden\n",
    "        )\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            out_features, max_len\n",
    "        )\n",
    "        \n",
    "    def forward(self, \n",
    "        aa_indices, mod_x\n",
    "    ):\n",
    "        mod_x = self.mod_nn(mod_x)\n",
    "        x = self.aa_emb(aa_indices)\n",
    "        return self.pos_encoder(torch.cat((x, mod_x), 2))\n",
    "\n",
    "class Input_AA_Mod_Charge_PositionalEncoding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Embed AA (128 ASCII codes), modification, and charge state\n",
    "    \"\"\"\n",
    "    def __init__(self, out_features, max_len=200):\n",
    "        super().__init__()\n",
    "        mod_hidden = 8\n",
    "        self.charge_dim = 2\n",
    "        self.mod_nn = Mod_Embedding_FixFirstK(mod_hidden)\n",
    "        self.aa_emb = ascii_embedding(\n",
    "            out_features-mod_hidden-self.charge_dim\n",
    "        )\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            out_features, max_len\n",
    "        )\n",
    "        \n",
    "    def forward(self, \n",
    "        aa_indices, mod_x, charges\n",
    "    ):\n",
    "        mod_x = self.mod_nn(mod_x)\n",
    "        x = self.aa_emb(aa_indices)\n",
    "        charge_x = charges.unsqueeze(1).repeat(\n",
    "            1, mod_x.size(1), self.charge_dim\n",
    "        )\n",
    "        return self.pos_encoder(torch.cat((x, mod_x,charge_x), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "Applying LSTMs to the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "        \n",
    "class Input_26AA_Mod_LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Applies an LSTM network to a AA (26 AA letters) sequence & modifications\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        out_features,\n",
    "        n_lstm_layers=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        mod_hidden = 8\n",
    "        self.mod_nn = Mod_Embedding_FixFirstK(mod_hidden)\n",
    "        self.lstm = SeqLSTM(\n",
    "            aa_embedding_size+mod_hidden,\n",
    "            out_features,\n",
    "            n_lstm_layers=n_lstm_layers, \n",
    "            bidirectional=True\n",
    "        )\n",
    "    def forward(self, aa_indices, mod_x):\n",
    "        mod_x = self.mod_nn(mod_x)\n",
    "        x = aa_one_hot(aa_indices, mod_x)\n",
    "        return self.lstm(x)\n",
    "#legacy\n",
    "InputAALSTM = Input_26AA_Mod_LSTM\n",
    "\n",
    "        \n",
    "class Input_26AA_Mod_Meta_LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Applies a LSTM network to a AA (26 AA letters) sequence and modifications,\n",
    "    and concatenates with 'meta' information (charge, nce, instrument_indices) \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        out_features,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        meta_dim = 4\n",
    "        mod_hidden = 8\n",
    "        self.mod_nn = Mod_Embedding_FixFirstK(mod_hidden)\n",
    "        self.meta_nn = Meta_Embedding(meta_dim)\n",
    "        self.nn = SeqLSTM(\n",
    "            aa_embedding_size+mod_hidden,\n",
    "            out_features-meta_dim,\n",
    "            rnn_layer=1, bidirectional=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, \n",
    "        aa_indices, mod_x, charges, NCEs, instrument_indices\n",
    "    ):\n",
    "        mod_x = self.mod_nn(mod_x)\n",
    "        x = aa_one_hot(aa_indices, mod_x)\n",
    "        x = self.nn(x)\n",
    "        meta_x = self.meta_nn(\n",
    "            charges, NCEs, instrument_indices\n",
    "        ).unsqueeze(1).repeat(1, mod_x.size(1), 1)\n",
    "        return torch.cat((x, meta_x), 2)\n",
    "#legacy\n",
    "InputAALSTM_cat_Meta = Input_26AA_Mod_Meta_LSTM\n",
    "\n",
    "\n",
    "class Input_26AA_Mod_Charge_LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Applies a LSTM network to a AA (26 AA letters) sequence and modifications, \n",
    "    and concatenates with charge state information\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        out_features,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.charge_dim = 2\n",
    "        mod_hidden = 8\n",
    "        self.mod_nn = Mod_Embedding_FixFirstK(mod_hidden)\n",
    "        self.nn = SeqLSTM(\n",
    "            aa_embedding_size+mod_hidden,\n",
    "            out_features-self.charge_dim,\n",
    "            rnn_layer=1, bidirectional=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, aa_indices, mod_x, charges):\n",
    "        mod_x = self.mod_nn(mod_x)\n",
    "        x = aa_one_hot(aa_indices, mod_x)\n",
    "        x = self.nn(x)\n",
    "        charge_x = charges.unsqueeze(1).repeat(\n",
    "            1, mod_x.size(1), self.charge_dim\n",
    "        )\n",
    "        return torch.cat((x, charge_x), 2)\n",
    "#legacy\n",
    "InputAALSTM_cat_Charge = Input_26AA_Mod_Charge_LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Seq Layers (or Output Layers)\n",
    "The 'Output' classes represent the output layers of the networks, meaning they take the hidden layer of a network as input, transform it into the output such as a spectrum, ccs value, rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Seq_Meta_LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Takes a hidden layer which processes the hidden tensor \n",
    "    as well as the 'meta' information of NCE, Instrument, Charge\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        meta_dim = 4\n",
    "        self.meta_nn = Meta_Embedding(meta_dim)\n",
    "        self.nn = SeqLSTM(\n",
    "            in_features+meta_dim,\n",
    "            out_features,\n",
    "            rnn_layer=1, bidirectional=False\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, charges, NCEs, instrument_indices):\n",
    "        meta_x = self.meta_nn(\n",
    "            charges, NCEs, instrument_indices\n",
    "        ).unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        return self.nn(torch.cat((x, meta_x), 2))\n",
    "#legacy\n",
    "OutputLSTM_cat_Meta = Seq_Meta_LSTM\n",
    "    \n",
    "class Seq_Meta_Linear(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    takes a hidden linear which processed the 'meta' information of NCE, Instrument, Charge\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        meta_dim = 4\n",
    "        self.meta_nn = Meta_Embedding(meta_dim)\n",
    "        self.nn = torch.nn.Linear(\n",
    "            in_features+meta_dim,\n",
    "            out_features,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, charges, NCEs, instrument_indices):\n",
    "        meta_x = self.meta_nn(\n",
    "            charges, NCEs, instrument_indices\n",
    "        ).unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        return self.nn(torch.cat((x, meta_x), 2))\n",
    "#legacy\n",
    "OutputLinear_cat_Meta = Seq_Meta_Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoders\n",
    "The encoder classes transform the features into a learned representation. Within the encoder, the `Input..` classes are used to transform the features into a network representation. Subsequently, Convolutional Neural Networks (CNNs) and/or Long Short-Term Memory (LSTM) Networks and/or Linear transformations are applied to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Encoder_26AA_Mod_LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Two LSTM layers on AA (26 AA letters) and modifications.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_features, n_lstm_layers=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_nn = Input_26AA_Mod_LSTM(out_features)\n",
    "        self.nn = SeqLSTM(\n",
    "            out_features, out_features, \n",
    "            rnn_layer=n_lstm_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, aa_indices, mod_x):\n",
    "        x = self.input_nn(aa_indices, mod_x)\n",
    "        x = self.nn(x)\n",
    "        return x\n",
    "\n",
    "#legacy\n",
    "Input_AA_LSTM_Encoder = Encoder_26AA_Mod_LSTM\n",
    "\n",
    "\n",
    "class Encoder_26AA_Mod_CNN_LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encode AAs (26 AA letters) and modifications by CNN and LSTM layers\n",
    "    \"\"\"\n",
    "    def __init__(self, out_features, n_lstm_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        mod_hidden = 8\n",
    "        self.mod_nn = Mod_Embedding_FixFirstK(mod_hidden)\n",
    "        input_dim = aa_embedding_size+mod_hidden\n",
    "        self.input_cnn = SeqCNN(input_dim)\n",
    "        self.hidden_nn = SeqLSTM(\n",
    "            input_dim*4, out_features, \n",
    "            rnn_layer=n_lstm_layers\n",
    "        ) #SeqCNN outputs 4*input_dim\n",
    "\n",
    "    def forward(self, aa_indices, mod_x):\n",
    "        mod_x = self.mod_nn(mod_x)\n",
    "        x = aa_one_hot(aa_indices, mod_x)\n",
    "        x = self.input_cnn(x)\n",
    "        x = self.hidden_nn(x)\n",
    "        return x\n",
    "\n",
    "#legacy\n",
    "Input_AA_CNN_Encoder = Encoder_26AA_Mod_CNN_LSTM\n",
    "\n",
    "class Encoder_26AA_Mod_CNN_LSTM_AttnSum(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encode AAs (26 AA letters) and modifications by CNN and LSTM layers, \n",
    "    then by 'SeqAttentionSum'.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_features, n_lstm_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        mod_hidden = 8\n",
    "        self.mod_nn = Mod_Embedding_FixFirstK(mod_hidden)\n",
    "\n",
    "        input_dim = aa_embedding_size+mod_hidden\n",
    "        self.input_cnn = SeqCNN(input_dim)\n",
    "        self.hidden_nn = SeqLSTM(\n",
    "            input_dim*4, out_features, \n",
    "            rnn_layer=n_lstm_layers\n",
    "        ) #SeqCNN outputs 4*input_dim\n",
    "        self.attn_sum = SeqAttentionSum(out_features)\n",
    "\n",
    "    def forward(self, aa_indices, mod_x):\n",
    "        mod_x = self.mod_nn(mod_x)\n",
    "        x = aa_one_hot(aa_indices, mod_x)\n",
    "        x = self.input_cnn(x)\n",
    "        x = self.hidden_nn(x)\n",
    "        x = self.attn_sum(x)\n",
    "        return x\n",
    "#legacy\n",
    "Input_AA_CNN_LSTM_Encoder = Encoder_26AA_Mod_CNN_LSTM_AttnSum\n",
    "\n",
    "class Encoder_AA_Mod_CNN_LSTM_AttnSum(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encode AAs (128 ASCII codes) and modifications by CNN and LSTM layers, \n",
    "    and then by 'SeqAttentionSum'.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_features, n_lstm_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        mod_hidden = 8\n",
    "        input_dim = out_features//4\n",
    "        self.aa_mod_embedding = AA_Mod_Embedding(\n",
    "            input_dim, mod_features=mod_hidden\n",
    "        )\n",
    "        self.input_cnn = SeqCNN(input_dim)\n",
    "        self.hidden_nn = SeqLSTM(\n",
    "            input_dim*4, out_features, \n",
    "            rnn_layer=n_lstm_layers\n",
    "        ) #SeqCNN outputs 4*input_dim\n",
    "        self.attn_sum = SeqAttentionSum(out_features)\n",
    "\n",
    "    def forward(self, aa_indices, mod_x):\n",
    "        x = self.aa_mod_embedding(aa_indices, mod_x)\n",
    "        x = self.input_cnn(x)\n",
    "        x = self.hidden_nn(x)\n",
    "        x = self.attn_sum(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder_AA_Mod_Transformer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    AAs (128 ASCII codes) and modifications embedded by CNN and LSTM layers, \n",
    "    then encoded by 'SeqAttentionSum'.\n",
    "    \"\"\"\n",
    "    def __init__(self,out_features,\n",
    "        dropout=0.1,\n",
    "        nlayers=4,\n",
    "        output_attentions=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        self.input_nn = Input_AA_Mod_PositionalEncoding(out_features)\n",
    "\n",
    "        self.output_attentions = output_attentions\n",
    "        self.encoder = Hidden_HFace_Transformer(\n",
    "            out_features, nlayers=nlayers, dropout=dropout,\n",
    "            output_attentions=output_attentions\n",
    "        )\n",
    "    def forward(self, aa_indices, mod_x):\n",
    "        in_x = self.dropout(self.input_nn(\n",
    "            aa_indices, mod_x\n",
    "        ))\n",
    "\n",
    "        x = self.encoder(in_x)\n",
    "        if self.output_attentions:\n",
    "            self.attentions = x[1]\n",
    "        else:\n",
    "            self.attentions = None\n",
    "        return x[0]\n",
    "\n",
    "class Encoder_AA_Mod_Transformer_AttnSum(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encode AAs (128 ASCII codes) and modifications by transformers.\n",
    "    \"\"\"\n",
    "    def __init__(self,out_features,\n",
    "        dropout=0.1,\n",
    "        nlayers=4,\n",
    "        output_attentions=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        self.encoder_nn = Encoder_AA_Mod_Transformer(\n",
    "            out_features, dropout=dropout, nlayers=nlayers,\n",
    "            output_attentions=output_attentions \n",
    "        )\n",
    "        self.attn_sum = SeqAttentionSum(out_features)\n",
    "        \n",
    "    def forward(self, aa_indices, mod_x):\n",
    "        x = self.encoder_nn(aa_indices, mod_x)\n",
    "        return self.dropout(self.attn_sum(x))\n",
    "\n",
    "class Encoder_AA_Mod_Charge_Transformer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encode AAs (128 ASCII codes), modifications and charge by transformers.\n",
    "    \"\"\"\n",
    "    def __init__(self,out_features,\n",
    "        dropout=0.1,\n",
    "        nlayers=4,\n",
    "        output_attentions=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.input_nn = Input_AA_Mod_Charge_PositionalEncoding(out_features)\n",
    "\n",
    "        self.output_attentions = output_attentions\n",
    "        self.encoder = Hidden_HFace_Transformer(\n",
    "            out_features, nlayers=nlayers, dropout=dropout,\n",
    "            output_attentions=output_attentions\n",
    "        )\n",
    "    def forward(self, aa_indices, mod_x, charges):\n",
    "        in_x = self.dropout(self.input_nn(\n",
    "            aa_indices, mod_x, charges\n",
    "        ))\n",
    "\n",
    "        x = self.encoder(in_x)\n",
    "        if self.output_attentions:\n",
    "            self.attentions = x[1]\n",
    "        else:\n",
    "            self.attentions = None\n",
    "        return x[0]\n",
    "\n",
    "class Encoder_AA_Mod_Charge_Transformer_AttnSum(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encode AAs (128 ASCII codes), modifications and charge by transformers, \n",
    "    and then by 'SeqAttentionSum'\n",
    "    \"\"\"\n",
    "    def __init__(self,out_features,\n",
    "        dropout=0.1,\n",
    "        nlayers=4,\n",
    "        output_attentions=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        self.encoder_nn = Encoder_AA_Mod_Charge_Transformer(\n",
    "            out_features, dropout=dropout, nlayers=nlayers,\n",
    "            output_attentions=output_attentions \n",
    "        )\n",
    "        self.attn_sum = SeqAttentionSum(out_features)\n",
    "    def forward(self, aa_indices, mod_x, charges):\n",
    "        x = self.encoder_nn(aa_indices, mod_x, charges)\n",
    "        return self.dropout(self.attn_sum(x))\n",
    "\n",
    "class Encoder_26AA_Mod_Charge_CNN_LSTM_AttnSum(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encode AAs (26 AA letters), modifications and charge by transformers, \n",
    "    and then by 'SeqAttentionSum'\n",
    "    \"\"\"\n",
    "    def __init__(self, out_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        mod_hidden = 8\n",
    "        self.mod_nn = Mod_Embedding_FixFirstK(mod_hidden)\n",
    "\n",
    "        input_dim = aa_embedding_size+mod_hidden+1\n",
    "        self.input_cnn = SeqCNN(input_dim)\n",
    "        self.hidden_nn = SeqLSTM(\n",
    "            input_dim*4, out_features, rnn_layer=2\n",
    "        ) #SeqCNN outputs 4*input_dim\n",
    "        self.attn_sum = SeqAttentionSum(out_features)\n",
    "\n",
    "    def forward(self, aa_indices, mod_x, charges):\n",
    "        mod_x = self.mod_nn(mod_x)\n",
    "        x = aa_one_hot(\n",
    "            aa_indices, mod_x, \n",
    "            charges.unsqueeze(1).repeat(1,mod_x.size(1),1)\n",
    "        )\n",
    "        x = self.input_cnn(x)\n",
    "        x = self.hidden_nn(x)\n",
    "        x = self.attn_sum(x)\n",
    "        return x\n",
    "\n",
    "#legacy\n",
    "Input_AA_CNN_LSTM_cat_Charge_Encoder = Encoder_26AA_Mod_Charge_CNN_LSTM_AttnSum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Decoder_LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Decode with LSTM\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden = 128\n",
    "        self.rnn = SeqLSTM(\n",
    "            in_features, out_features,\n",
    "            rnn_layer=1, bidirectional=False,\n",
    "        )\n",
    "\n",
    "        self.output_nn = torch.nn.Linear(\n",
    "            hidden, out_features, bias=False\n",
    "        )\n",
    "    \n",
    "    def forward(self, x:torch.tensor, output_len):\n",
    "        x = self.rnn(\n",
    "            x.unsqueeze(1).repeat(1,output_len,1)\n",
    "        )\n",
    "        x = self.output_nn(x)\n",
    "        return x\n",
    "#legacy\n",
    "SeqLSTMDecoder = Decoder_LSTM\n",
    "\n",
    "class Decoder_GRU(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Decode with GRU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden = 128\n",
    "        self.rnn = SeqGRU(\n",
    "            in_features, out_features,\n",
    "            rnn_layer=1, bidirectional=False,\n",
    "        )\n",
    "\n",
    "        self.output_nn = torch.nn.Linear(\n",
    "            hidden, out_features, bias=False\n",
    "        )\n",
    "    \n",
    "    def forward(self, x:torch.tensor, output_len):\n",
    "        x = self.rnn(\n",
    "            x.unsqueeze(1).repeat(1,output_len,1)\n",
    "        )\n",
    "        x = self.output_nn(x)\n",
    "        return x\n",
    "#legacy\n",
    "SeqGRUDecoder = Decoder_GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Decoder_Linear(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Decode w linear NN\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features, 64),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(64, out_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "#legacy\n",
    "LinearDecoder = Decoder_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from peptdeep.pretrained_models import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "model_mgr = ModelManager(device='cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
